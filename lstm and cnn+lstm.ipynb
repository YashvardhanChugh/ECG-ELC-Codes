{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:58:38.890583Z","iopub.execute_input":"2025-07-15T18:58:38.890842Z","iopub.status.idle":"2025-07-15T19:00:32.157176Z","shell.execute_reply.started":"2025-07-15T18:58:38.890822Z","shell.execute_reply":"2025-07-15T19:00:32.156494Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy pandas wfdb scikit-learn torch torchvision torchaudio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T19:00:32.158416Z","iopub.execute_input":"2025-07-15T19:00:32.158884Z","iopub.status.idle":"2025-07-15T19:01:52.526122Z","shell.execute_reply.started":"2025-07-15T19:00:32.158859Z","shell.execute_reply":"2025-07-15T19:01:52.525207Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import pandas as pd\n# import wfdb\n# import ast\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report\n\n# # Configuration\n# class Config:\n#     ECG_FOLDER = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\n#     METADATA_FILE = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/ptbxl_database.csv\"\n#     SCP_FILE = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/scp_statements.csv\"\n#     SIGNAL_LENGTH = 1000\n#     NUM_LEADS = 12\n#     BATCH_SIZE = 32\n#     EPOCHS = 55\n#     LR = 1e-3\n#     HIDDEN_SIZE = 128\n#     NUM_LAYERS = 2\n#     DROPOUT = 0.3\n#     TEST_SIZE = 0.2\n#     VAL_SIZE = 0.1\n#     RANDOM_STATE = 42\n\n# # Dataset\n# class ECGDataset(Dataset):\n#     def __init__(self, df, config):\n#         self.df = df.reset_index(drop=True)\n#         self.config = config\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n#         path = os.path.join(self.config.ECG_FOLDER, row['filename_lr'])\n#         sig, _ = wfdb.rdsamp(path)\n#         sig = sig.T\n#         # Normalize\n#         sig = (sig - sig.mean(axis=1, keepdims=True)) / (sig.std(axis=1, keepdims=True) + 1e-8)\n#         # Pad/Trim\n#         if sig.shape[1] < self.config.SIGNAL_LENGTH:\n#             pad = self.config.SIGNAL_LENGTH - sig.shape[1]\n#             sig = np.pad(sig, ((0,0),(0,pad)))\n#         else:\n#             sig = sig[:, :self.config.SIGNAL_LENGTH]\n#         return torch.tensor(sig.T, dtype=torch.float32), torch.tensor(row['class_id'], dtype=torch.long)\n\n# # LSTM Model\n# class LSTMClassifier(nn.Module):\n#     def __init__(self, config, num_classes):\n#         super().__init__()\n#         self.lstm = nn.LSTM(\n#             input_size=config.NUM_LEADS,\n#             hidden_size=config.HIDDEN_SIZE,\n#             num_layers=config.NUM_LAYERS,\n#             batch_first=True,\n#             dropout=config.DROPOUT,\n#             bidirectional=True\n#         )\n#         self.classifier = nn.Sequential(\n#             nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE),\n#             nn.ReLU(),\n#             nn.Dropout(config.DROPOUT),\n#             nn.Linear(config.HIDDEN_SIZE, num_classes)\n#         )\n\n#     def forward(self, x):\n#         out, _ = self.lstm(x)\n#         out = out[:, -1, :]  # Last timestep\n#         return self.classifier(out)\n\n# # Preprocessing\n# def preprocess(config):\n#     df = pd.read_csv(config.METADATA_FILE)\n#     scp_df = pd.read_csv(config.SCP_FILE, index_col=0)\n\n#     def get_superclass(code_str):\n#         d = ast.literal_eval(code_str)\n#         filtered = {k: v for k, v in d.items() if k in scp_df.index and scp_df.loc[k, 'diagnostic'] == 1}\n#         if not filtered: return 'NORM'\n#         best = max(filtered, key=filtered.get)\n#         return scp_df.loc[best, 'diagnostic_class'] or 'NORM'\n\n#     df['diagnostic_superclass'] = df['scp_codes'].apply(get_superclass)\n#     le = LabelEncoder()\n#     df['class_id'] = le.fit_transform(df['diagnostic_superclass'])\n\n#     train_idx, test_idx = train_test_split(df.index, test_size=config.TEST_SIZE,\n#                                            stratify=df['class_id'], random_state=config.RANDOM_STATE)\n#     train_meta = df.loc[train_idx].reset_index(drop=True)\n#     test_meta = df.loc[test_idx].reset_index(drop=True)\n\n#     train_idx, val_idx = train_test_split(train_meta.index, test_size=config.VAL_SIZE,\n#                                           stratify=train_meta['class_id'], random_state=config.RANDOM_STATE)\n\n#     return train_meta.loc[train_idx], train_meta.loc[val_idx], test_meta, le\n\n# # Training + Evaluation\n# def train_model():\n#     config = Config()\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     train_df, val_df, test_df, label_encoder = preprocess(config)\n#     num_classes = len(label_encoder.classes_)\n\n#     model = LSTMClassifier(config, num_classes).to(device)\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n#     criterion = nn.CrossEntropyLoss()\n\n#     train_loader = DataLoader(ECGDataset(train_df, config), batch_size=config.BATCH_SIZE, shuffle=True)\n#     val_loader = DataLoader(ECGDataset(val_df, config), batch_size=config.BATCH_SIZE)\n#     test_loader = DataLoader(ECGDataset(test_df, config), batch_size=config.BATCH_SIZE)\n\n#     print(f\"Training on {device}\")\n#     for epoch in range(1, config.EPOCHS + 1):\n#         model.train()\n#         total_loss, total_correct, total_samples = 0, 0, 0\n#         for x, y in train_loader:\n#             x, y = x.to(device), y.to(device)\n#             optimizer.zero_grad()\n#             preds = model(x)\n#             loss = criterion(preds, y)\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item() * y.size(0)\n#             total_correct += (preds.argmax(1) == y).sum().item()\n#             total_samples += y.size(0)\n#         train_loss = total_loss / total_samples\n#         train_acc = total_correct / total_samples\n\n#         model.eval()\n#         with torch.no_grad():\n#             val_loss, val_correct, val_total = 0, 0, 0\n#             for x, y in val_loader:\n#                 x, y = x.to(device), y.to(device)\n#                 preds = model(x)\n#                 loss = criterion(preds, y)\n#                 val_loss += loss.item() * y.size(0)\n#                 val_correct += (preds.argmax(1) == y).sum().item()\n#                 val_total += y.size(0)\n#         val_loss /= val_total\n#         val_acc = val_correct / val_total\n#         print(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n#     # Final test evaluation\n#     model.eval()\n#     all_preds, all_labels = [], []\n#     with torch.no_grad():\n#         for x, y in test_loader:\n#             x = x.to(device)\n#             out = model(x)\n#             preds = out.argmax(1).cpu().numpy()\n#             all_preds.extend(preds)\n#             all_labels.extend(y.numpy())\n#     print(\"\\nFinal Classification Report:\")\n#     print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n\n# if __name__ == \"__main__\":\n#     train_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T19:01:52.527218Z","iopub.execute_input":"2025-07-15T19:01:52.527518Z","iopub.status.idle":"2025-07-15T19:01:52.533986Z","shell.execute_reply.started":"2025-07-15T19:01:52.527486Z","shell.execute_reply":"2025-07-15T19:01:52.533461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport wfdb\nimport ast\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\n# Configuration\nclass Config:\n    ECG_FOLDER = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\n    METADATA_FILE = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/ptbxl_database.csv\"\n    SCP_FILE = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/scp_statements.csv\"\n    SIGNAL_LENGTH = 1000\n    NUM_LEADS = 12\n    BATCH_SIZE = 32\n    EPOCHS = 55\n    LR = 1e-3\n    # LSTM specific\n    HIDDEN_SIZE = 128\n    NUM_LAYERS = 2\n    # CNN specific\n    CNN_OUPUT_CHANNELS = 128 # Should be the input to the LSTM\n    # Shared\n    DROPOUT = 0.3\n    # Data splitting\n    TEST_SIZE = 0.2\n    VAL_SIZE = 0.1\n    RANDOM_STATE = 42\n\n# Dataset class remains unchanged\nclass ECGDataset(Dataset):\n    def __init__(self, df, config):\n        self.df = df.reset_index(drop=True)\n        self.config = config\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = os.path.join(self.config.ECG_FOLDER, row['filename_lr'])\n        # Use wfdb.rdsamp to read both the signal and metadata\n        sig, _ = wfdb.rdsamp(path)\n        sig = sig.T # Transpose to get (12, 5000)\n        # Normalize each lead independently\n        sig = (sig - sig.mean(axis=1, keepdims=True)) / (sig.std(axis=1, keepdims=True) + 1e-8)\n        # Pad or trim the signal to the desired length\n        if sig.shape[1] < self.config.SIGNAL_LENGTH:\n            pad_width = self.config.SIGNAL_LENGTH - sig.shape[1]\n            sig = np.pad(sig, ((0,0),(0,pad_width)))\n        else:\n            sig = sig[:, :self.config.SIGNAL_LENGTH]\n        # Transpose back for LSTM: (SIGNAL_LENGTH, NUM_LEADS)\n        return torch.tensor(sig.T, dtype=torch.float32), torch.tensor(row['class_id'], dtype=torch.long)\n\n# NEW: Hybrid CNN-LSTM Model\nclass CNN_LSTM_Classifier(nn.Module):\n    def __init__(self, config, num_classes):\n        super().__init__()\n        # 1D CNN Feature Extractor\n        self.cnn = nn.Sequential(\n            # Conv Block 1\n            nn.Conv1d(in_channels=config.NUM_LEADS, out_channels=64, kernel_size=7, stride=1, padding=3),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2),\n            # Conv Block 2\n            nn.Conv1d(in_channels=64, out_channels=config.CNN_OUPUT_CHANNELS, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n\n        # Bidirectional LSTM\n        self.lstm = nn.LSTM(\n            input_size=config.CNN_OUPUT_CHANNELS, # Input features are the output channels of the CNN\n            hidden_size=config.HIDDEN_SIZE,\n            num_layers=config.NUM_LAYERS,\n            batch_first=True,\n            dropout=config.DROPOUT,\n            bidirectional=True\n        )\n\n        # Classifier Head\n        self.classifier = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE), # *2 for bidirectional\n            nn.ReLU(),\n            nn.Dropout(config.DROPOUT),\n            nn.Linear(config.HIDDEN_SIZE, num_classes)\n        )\n\n    def forward(self, x):\n        # Input shape for CNN: (batch_size, num_leads, signal_length)\n        # Current input x shape: (batch_size, signal_length, num_leads)\n        # Permute to match Conv1d expectation\n        x = x.permute(0, 2, 1)\n\n        # Pass through CNN\n        x = self.cnn(x) # Shape: (batch_size, cnn_output_channels, new_signal_length)\n\n        # Input shape for LSTM: (batch_size, sequence_length, features)\n        # Permute again to match LSTM expectation\n        x = x.permute(0, 2, 1)\n\n        # Pass through LSTM\n        out, _ = self.lstm(x)\n\n        # Get the output of the last time step\n        out = out[:, -1, :]\n\n        # Pass through the classifier\n        return self.classifier(out)\n\n# Preprocessing function remains unchanged\ndef preprocess(config):\n    df = pd.read_csv(config.METADATA_FILE)\n    scp_df = pd.read_csv(config.SCP_FILE, index_col=0)\n\n    # Convert scp_codes from string to dictionary\n    df['scp_codes'] = df['scp_codes'].apply(ast.literal_eval)\n\n    def get_diagnostic_superclass(scp_codes):\n        diagnostic_classes = []\n        for code, confidence in scp_codes.items():\n            if code in scp_df.index and scp_df.loc[code, 'diagnostic'] == 1:\n                superclass = scp_df.loc[code, 'diagnostic_class']\n                if not pd.isna(superclass):\n                    diagnostic_classes.append(superclass)\n        \n        if not diagnostic_classes:\n            return 'NORM' # Default to NORM if no diagnostic class is found\n        \n        # Return the most frequent diagnostic class if multiple are present\n        return max(set(diagnostic_classes), key=diagnostic_classes.count)\n\n    df['diagnostic_superclass'] = df['scp_codes'].apply(get_diagnostic_superclass)\n    \n    le = LabelEncoder()\n    df['class_id'] = le.fit_transform(df['diagnostic_superclass'])\n\n    # Splitting data\n    train_val_df, test_df = train_test_split(df, test_size=config.TEST_SIZE,\n                                             stratify=df['class_id'], random_state=config.RANDOM_STATE)\n    train_df, val_df = train_test_split(train_val_df, test_size=config.VAL_SIZE / (1 - config.TEST_SIZE),\n                                        stratify=train_val_df['class_id'], random_state=config.RANDOM_STATE)\n\n    return train_df, val_df, test_df, le\n\n\n# Training + Evaluation function updated to use the new model\ndef train_model():\n    config = Config()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_df, val_df, test_df, label_encoder = preprocess(config)\n    num_classes = len(label_encoder.classes_)\n\n    # Use the new CNN_LSTM_Classifier\n    model = CNN_LSTM_Classifier(config, num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.LR)\n    criterion = nn.CrossEntropyLoss()\n\n    # DataLoaders remain the same\n    train_loader = DataLoader(ECGDataset(train_df, config), batch_size=config.BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(ECGDataset(val_df, config), batch_size=config.BATCH_SIZE, shuffle=False)\n    test_loader = DataLoader(ECGDataset(test_df, config), batch_size=config.BATCH_SIZE, shuffle=False)\n\n    print(f\"Training on {device} using CNN-LSTM model...\")\n    for epoch in range(1, config.EPOCHS + 1):\n        model.train()\n        total_loss, total_correct, total_samples = 0, 0, 0\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            preds = model(x)\n            loss = criterion(preds, y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * y.size(0)\n            total_correct += (preds.argmax(1) == y).sum().item()\n            total_samples += y.size(0)\n        train_loss = total_loss / total_samples\n        train_acc = total_correct / total_samples\n\n        model.eval()\n        with torch.no_grad():\n            val_loss, val_correct, val_total = 0, 0, 0\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                preds = model(x)\n                loss = criterion(preds, y)\n                val_loss += loss.item() * y.size(0)\n                val_correct += (preds.argmax(1) == y).sum().item()\n                val_total += y.size(0)\n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        print(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n    # Final test evaluation\n    print(\"\\n--- Final Test Evaluation ---\")\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            out = model(x)\n            preds = out.argmax(1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(y.numpy())\n            \n    print(\"\\nFinal Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T19:01:52.535737Z","iopub.execute_input":"2025-07-15T19:01:52.535933Z"}},"outputs":[],"execution_count":null}]}