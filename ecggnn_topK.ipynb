{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T18:53:20.128996Z","iopub.execute_input":"2025-06-29T18:53:20.129171Z","iopub.status.idle":"2025-06-29T18:55:25.613050Z","shell.execute_reply.started":"2025-06-29T18:53:20.129137Z","shell.execute_reply":"2025-06-29T18:55:25.612330Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-geometric pywavelets wfdb scikit-learn matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T18:55:25.614433Z","iopub.execute_input":"2025-06-29T18:55:25.615059Z","iopub.status.idle":"2025-06-29T18:55:30.868632Z","shell.execute_reply.started":"2025-06-29T18:55:25.615034Z","shell.execute_reply":"2025-06-29T18:55:30.867923Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data, Dataset, Batch\nfrom torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\nfrom torch.utils.data import DataLoader\nimport wfdb\nimport pywt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Dataset\nclass PTBXLGraphDataset(Dataset):\n    def __init__(self, meta_df, ecg_folder, wavelet='db4', level=4, threshold_factor=0.1):\n        self.meta = meta_df\n        self.ecg_folder = ecg_folder\n        self.wavelet = wavelet\n        self.level = level\n        self.threshold_factor = threshold_factor\n        num_leads = 12\n        edges = [(i, j) for i in range(num_leads) for j in range(num_leads) if i != j]\n        self.edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n    def __len__(self):\n        return len(self.meta)\n\n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        rec = os.path.join(self.ecg_folder, row['filename_lr'])\n        sig, _ = wfdb.rdsamp(rec)\n        sig = sig.T\n\n        node_features = []\n        for lead in sig:\n            lead = (lead - np.mean(lead)) / (np.std(lead) + 1e-8)\n            coeffs = pywt.wavedec(lead, self.wavelet, level=self.level)\n            coeffs_thresh = [pywt.threshold(c, self.threshold_factor * np.max(np.abs(c)), mode='soft') if np.max(np.abs(c)) > 0 else c for c in coeffs]\n            denoised = pywt.waverec(coeffs_thresh, self.wavelet)\n            denoised = denoised[:1000] if len(denoised) > 1000 else np.pad(denoised, (0, 1000 - len(denoised)), 'constant')\n            node_features.append(denoised)\n\n        node_features = torch.tensor(np.array(node_features), dtype=torch.float32)\n        y = torch.tensor(row['superclass_id'], dtype=torch.long)  # Changed to superclass_id\n        return Data(x=node_features, edge_index=self.edge_index, y=y)\n\n# CNN Backbone\nclass ResNetECG(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(12, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm1d(64), nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm1d(128), nn.ReLU()\n        )\n        self.gap = nn.AdaptiveAvgPool1d(1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return self.gap(x).squeeze(-1)\n\n# GNN Module\nclass ECGGNNModule(nn.Module):\n    def __init__(self, in_channels=1000, hidden_channels=64):\n        super().__init__()\n        self.gcn1 = GCNConv(in_channels, hidden_channels)\n        self.pool1 = TopKPooling(hidden_channels, ratio=0.8)\n        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n        self.pool2 = TopKPooling(hidden_channels, ratio=0.8)\n        self.lin = nn.Linear(hidden_channels, hidden_channels)\n\n    def forward(self, x, edge_index, batch):\n        x = F.relu(self.gcn1(x, edge_index))\n        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n        x = F.relu(self.gcn2(x, edge_index))\n        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n        x_pooled = global_mean_pool(x, batch)\n        return self.lin(x_pooled)\n\n# Full ECGNN\nclass ECGNN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.cnn = ResNetECG()\n        self.gnn = ECGGNNModule()\n        self.fc = nn.Linear(128 + 64, num_classes)\n\n    def forward(self, data):\n        batch_size = data.num_graphs\n        nodes_per_graph = 12\n        x_reshaped = data.x.view(batch_size, nodes_per_graph, -1)\n        x_cnn = x_reshaped\n        cnn_out = self.cnn(x_cnn)\n        gnn_out = self.gnn(data.x, data.edge_index, data.batch)\n        return self.fc(torch.cat([cnn_out, gnn_out], dim=1))\n\n# Note: The following code assumes train_meta, val_meta, and test_meta are already defined\n# You'll need to add your data loading code here\n\n# Data Preparation\ntrain_meta, val_meta = train_test_split(\n    train_meta, test_size=0.2, stratify=train_meta['superclass'], random_state=42\n)\n\nsuperclass_le = LabelEncoder()\nsuperclass_le.fit(train_meta['superclass'])\ntrain_meta['superclass_id'] = superclass_le.transform(train_meta['superclass'])\nval_meta['superclass_id'] = superclass_le.transform(val_meta['superclass'])\ntest_meta['superclass_id'] = superclass_le.transform(test_meta['superclass'])\n\n# Datasets and Loaders\necg_folder = '/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1'\ntrain_ds = PTBXLGraphDataset(train_meta, ecg_folder)\nval_ds = PTBXLGraphDataset(val_meta, ecg_folder)\ntest_ds = PTBXLGraphDataset(test_meta, ecg_folder)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=Batch.from_data_list)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=Batch.from_data_list)\ntest_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=Batch.from_data_list)\n\n# Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ECGNN(num_classes=len(superclass_le.classes_)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# Training Loop with Periodic Validation\nfor epoch in range(1, 21):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n    train_acc = correct / total\n    print(f\"Epoch {epoch} | Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n    if epoch % 10 == 0:\n        model.eval()\n        val_correct, val_total = 0, 0\n        with torch.no_grad():\n            for data in val_loader:\n                data = data.to(device)\n                out = model(data)\n                pred = out.argmax(dim=1)\n                val_correct += (pred == data.y).sum().item()\n                val_total += data.y.size(0)\n        val_acc = val_correct / val_total\n        print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f}\")\n\n# Final Test Evaluation\nmodel.eval()\ny_true, y_pred = [], []\nwith torch.no_grad():\n    for data in test_loader:\n        data = data.to(device)\n        out = model(data)\n        preds = out.argmax(dim=1)\n        y_true.extend(data.y.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\ntest_acc = accuracy_score(y_true, y_pred)\nprint(f\"Final Test Accuracy: {test_acc:.4f}\")\nprint(\"\\nTest Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=superclass_le.classes_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from torch.utils.data import Dataset, DataLoader\n# from torch_geometric.data import Data, Batch\n# from torch_geometric.nn import GATConv, GraphNorm, Set2Set\n# import wfdb\n# import pywt\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report\n# import ast\n# import os\n# from torch.optim.lr_scheduler import CosineAnnealingLR\n\n# # Load metadata and diagnostic codes\n# df = pd.read_csv('/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/ptbxl_database.csv')\n# scp_statements = pd.read_csv('/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/scp_statements.csv', index_col=0)\n\n# def extract_dominant_superclass(scp_dict):\n#     filtered = {k: v for k, v in scp_dict.items() if k in scp_statements.index and scp_statements.loc[k, 'diagnostic'] == 1}\n#     if not filtered:\n#         return 'NORM'\n#     dominant_code = max(filtered, key=filtered.get)\n#     superclass = scp_statements.loc[dominant_code, 'diagnostic_class']\n#     return superclass if superclass else 'NORM'\n\n# df['scp_codes'] = df['scp_codes'].apply(ast.literal_eval)\n# df['diagnostic_superclass'] = df['scp_codes'].apply(extract_dominant_superclass)\n# le = LabelEncoder()\n# df['class_id'] = le.fit_transform(df['diagnostic_superclass'])\n\n# # Split data into train, validation, and test sets\n# train_idx, test_idx = train_test_split(df.index, test_size=0.2, stratify=df['class_id'], random_state=42)\n# train_meta = df.loc[train_idx].reset_index(drop=True)\n# test_meta = df.loc[test_idx].reset_index(drop=True)\n# train_idx, val_idx = train_test_split(train_meta.index, test_size=0.1, stratify=train_meta['class_id'], random_state=42)\n# train_meta_new = train_meta.loc[train_idx].reset_index(drop=True)\n# val_meta = train_meta.loc[val_idx].reset_index(drop=True)\n\n# # Dataset class with adaptive denoising and augmentation\n# class PTBXLGraphDataset(Dataset):\n#     def __init__(self, meta_df, ecg_folder, wavelet='db4', level=4, train=True):\n#         self.meta = meta_df\n#         self.ecg_folder = ecg_folder\n#         self.wavelet = wavelet\n#         self.level = level\n#         self.train_mode = train\n#         self.edge_index = self.build_edge_index()\n\n#     def build_edge_index(self):\n#         num_leads = 12\n#         edges = [(i, j) for i in range(num_leads) for j in range(num_leads) if i != j]\n#         return torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n#     def __len__(self):\n#         return len(self.meta)\n\n#     def __getitem__(self, idx):\n#         row = self.meta.iloc[idx]\n#         rec_path = os.path.join(self.ecg_folder, row['filename_lr'])\n#         sig, _ = wfdb.rdsamp(rec_path)\n#         sig = sig.T  # Shape: (12, 1000)\n\n#         if self.train_mode:\n#             num_drop = np.random.randint(0, 3)\n#             drop_leads = np.random.choice(12, size=num_drop, replace=False)\n#         else:\n#             drop_leads = []\n\n#         node_features = []\n#         for i, lead in enumerate(sig):\n#             if self.train_mode:\n#                 shift = np.random.randint(-50, 51)\n#                 lead = np.roll(lead, shift)\n#                 scale = np.random.uniform(0.9, 1.1)\n#                 lead = lead * scale\n#                 noise = np.random.normal(0, 0.01, lead.shape)\n#                 lead = lead + noise\n\n#             lead = (lead - np.mean(lead)) / (np.std(lead) + 1e-8)\n#             coeffs = pywt.wavedec(lead, self.wavelet, level=self.level)\n#             if len(coeffs[-1]) > 0:\n#                 sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n#                 threshold = sigma * np.sqrt(2 * np.log(len(lead)))\n#                 coeffs_thresh = [c if i == 0 else pywt.threshold(c, threshold, mode='soft') for i, c in enumerate(coeffs)]\n#                 denoised = pywt.waverec(coeffs_thresh, self.wavelet)\n#             else:\n#                 denoised = lead\n\n#             denoised = denoised[:1000] if len(denoised) > 1000 else np.pad(denoised, (0, 1000 - len(denoised)), 'constant')\n#             node_features.append(np.zeros(1000) if i in drop_leads else denoised)\n\n#         x = torch.tensor(np.array(node_features), dtype=torch.float32)\n#         y = torch.tensor(row['class_id'], dtype=torch.long)\n#         return Data(x=x, edge_index=self.edge_index, y=y)\n\n# # Residual CNN block\n# class ResidualBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size):\n#         super().__init__()\n#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n#         self.bn1 = nn.BatchNorm1d(out_channels)\n#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n#         self.bn2 = nn.BatchNorm1d(out_channels)\n#         self.relu = nn.ReLU()\n#         self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n\n#     def forward(self, x):\n#         identity = self.residual(x)\n#         out = self.relu(self.bn1(self.conv1(x)))\n#         out = self.bn2(self.conv2(out))\n#         return self.relu(out + identity)\n\n# # CNN for feature extraction\n# class NodeCNN(nn.Module):\n#     def __init__(self, signal_length, out_dim):\n#         super().__init__()\n#         self.block1 = ResidualBlock(1, 32, 5)\n#         self.block2 = ResidualBlock(32, 64, 5)\n#         self.block3 = ResidualBlock(64, 128, 3)\n#         self.block4 = ResidualBlock(128, 256, 3)\n#         self.block5 = ResidualBlock(256, 512, 3)\n#         self.pool = nn.AdaptiveAvgPool1d(1)\n#         self.fc = nn.Linear(512, out_dim)\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, x):\n#         x = self.block1(x)\n#         x = self.dropout(x)\n#         x = self.block2(x)\n#         x = self.dropout(x)\n#         x = self.block3(x)\n#         x = self.dropout(x)\n#         x = self.block4(x)\n#         x = self.dropout(x)\n#         x = self.block5(x)\n#         x = self.dropout(x)\n#         x = self.pool(x).squeeze(-1)\n#         return self.fc(x)\n\n# # Combined CNN and GNN model\n# class CNN_GNN_Model(nn.Module):\n#     def __init__(self, signal_length, cnn_out_dim, gnn_hidden, num_classes):\n#         super().__init__()\n#         self.node_cnn = NodeCNN(signal_length, cnn_out_dim)\n#         self.gnn1 = GATConv(cnn_out_dim, gnn_hidden, heads=4, concat=True)\n#         self.norm1 = GraphNorm(gnn_hidden * 4)\n#         self.gnn2 = GATConv(gnn_hidden * 4, gnn_hidden, heads=1, concat=False)\n#         self.norm2 = GraphNorm(gnn_hidden)\n#         self.pool = Set2Set(gnn_hidden, processing_steps=3)\n#         self.fc = nn.Linear(gnn_hidden * 2, num_classes)\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, data):\n#         x, edge_index, batch = data.x, data.edge_index, data.batch\n#         x = x.unsqueeze(1)  # (N, 1, 1000)\n#         x = self.node_cnn(x)  # (N, cnn_out_dim)\n#         x = F.relu(self.norm1(self.gnn1(x, edge_index)))\n#         x = self.dropout(x)\n#         x = F.relu(self.norm2(self.gnn2(x, edge_index)))\n#         x = self.dropout(x)\n#         x = self.pool(x, batch)\n#         return self.fc(x)\n\n# # Focal Loss for handling class imbalance\n# class FocalLoss(nn.Module):\n#     def __init__(self, alpha=1.0, gamma=2.0):\n#         super().__init__()\n#         self.alpha = alpha\n#         self.gamma = gamma\n\n#     def forward(self, inputs, targets):\n#         ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n#         pt = torch.exp(-ce_loss)\n#         focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n#         return focal_loss.mean()\n\n# # Data preparation\n# ecg_folder = '/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n# train_dataset = PTBXLGraphDataset(train_meta_new, ecg_folder, train=True)\n# val_dataset = PTBXLGraphDataset(val_meta, ecg_folder, train=False)\n# test_dataset = PTBXLGraphDataset(test_meta, ecg_folder, train=False)\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=Batch.from_data_list)\n# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=Batch.from_data_list)\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=Batch.from_data_list)\n\n# # Model setup\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = CNN_GNN_Model(signal_length=1000, cnn_out_dim=128, gnn_hidden=64, num_classes=len(le.classes_)).to(device)\n# criterion = FocalLoss()\n# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n# scheduler = CosineAnnealingLR(optimizer, T_max=50)\n\n# # Training and evaluation functions\n# def train_epoch():\n#     model.train()\n#     total_loss = 0\n#     correct = 0\n#     total = 0\n#     for data in train_loader:\n#         data = data.to(device)\n#         optimizer.zero_grad()\n#         out = model(data)\n#         loss = criterion(out, data.y)\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item() * data.y.size(0)\n#         preds = out.argmax(dim=1)\n#         correct += (preds == data.y).sum().item()\n#         total += data.y.size(0)\n#     return total_loss / total, correct / total\n\n# def evaluate(loader):\n#     model.eval()\n#     total_loss = 0\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for data in loader:\n#             data = data.to(device)\n#             out = model(data)\n#             loss = criterion(out, data.y)\n#             total_loss += loss.item() * data.y.size(0)\n#             preds = out.argmax(dim=1)\n#             correct += (preds == data.y).sum().item()\n#             total += data.y.size(0)\n#     return total_loss / total, correct / total\n\n# # Training loop with early stopping\n# best_val_acc = 0\n# patience = 10\n# counter = 0\n# for epoch in range(1, 101):\n#     train_loss, train_acc = train_epoch()\n#     val_loss, val_acc = evaluate(val_loader)\n#     print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n#     scheduler.step()\n#     if val_acc > best_val_acc:\n#         best_val_acc = val_acc\n#         torch.save(model.state_dict(), 'best_model.pt')\n#         counter = 0\n#     else:\n#         counter += 1\n#     if counter >= patience:\n#         print(\"Early stopping triggered\")\n#         break\n\n# # Test evaluation\n# model.load_state_dict(torch.load('best_model.pt'))\n# test_loss, test_acc = evaluate(test_loader)\n# print(f\"Final Test Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\")\n\n# y_true, y_pred = [], []\n# with torch.no_grad():\n#     for data in test_loader:\n#         data = data.to(device)\n#         out = model(data)\n#         preds = out.argmax(dim=1)\n#         y_true.extend(data.y.cpu().numpy())\n#         y_pred.extend(preds.cpu().numpy())\n\n# report = classification_report(y_true, y_pred, target_names=le.classes_, digits=4)\n# print(\"\\nClassification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T18:55:30.869663Z","iopub.execute_input":"2025-06-29T18:55:30.869957Z"}},"outputs":[],"execution_count":null}]}